# Chapter 4: Clustering and classification


```{r}
date()

```

**_My fourth week in short_**

- The week started by doing the peer-reviews. It was nice to see what other students have done!
- For warming up I did first the data wrangling exercise (preparing the data for the 5th week exercise)
- Next I started to read the related chapters and figuring out the DataCamp exercises (and their bugs)
- Finally I did the RStudio exercises
- This week's exercises were a bit annoying as I think the study materials did not provide adequate information to work with the exercises

## 4.1 Data
```{r message=FALSE, warning=FALSE}
# access to packages
library(MASS)
library(dplyr)
library(tidyr)
library(ggplot2)
library(corrplot)

# get the data (included in MASS)
data(Boston)

# structure of the data
str(Boston)
dim(Boston)

```

The Boston data set utilized here includes 'Housing Values in Suburbs of Boston', that is different variables relating housing in the city of Boston. The data consists of 506 observations or 14 numeric (or integer) variables (i.e. 506 rows and 14 columns). Description of the variables can be seen from [the description of the data set](https://stat.ethz.ch/R-manual/R-devel/library/MASS/html/Boston.html)

## 4.2 Overview of the Boston data

```{r}
summary(Boston)

# let's use gather from tidyr-package; gather returns key-value pairs of variables
# draw a bar plot of each variable
gather(Boston) %>% 
  ggplot(aes(value)) + 
  facet_wrap("key", scales = "free") +
  geom_histogram()

# construct a correlation matrix and round the results
cor_matrix <-cor(Boston) %>%
    round(digits = 2)

# visualize the correlation matrix
corrplot(cor_matrix, method = "circle", type = "upper", cl.pos = "r", tl.pos = "d", tl.cex = 0.8)

```

*Show a graphical overview of the data and show summaries of the variables in the data Describe and interpret the outputs, commenting on the distributions of the variables and the relationships between them* (CHECK)

Summaries of the variables and the histograms presented above illustrates that the variables in the data set have skewed distributions and the variable also have quite different scales compared to each other. 

The correlation matrix shows that many of variables are highly correlated. The correlation coefficients vary from 1 to -1. A positive coefficient indicate that high values of variable 'X' are associated with the high values of variable 'Y'. Respectively negative coefficient indicate that high values of 'X' are associated with low values of 'Y'.

## 4.3 Standardized dataset 

First I'll scale the Boston data, that is standardize each variable by its scale and the standardized variables as a data.frame instead of a matrix. When variables are centered, their mean is adjusted to zero as can be seen from the summaries of the scaled variables which are shown below.

After scaling the data, I will create a new factor variable from the 'crime rate per capita' variable and use the quantiles as cut points. A summary of the new 'crime' variable can be found below.

Finally, I will split the scaled data into a training and testing data sets, which are then used in the next subchapter.

```{r}

# scale the Boston data and transform the matrix to data.frame
boston_scaled <- Boston %>%
  scale() %>%
  as.data.frame()
summary(boston_scaled)

# pick the quantiles of crim
bins <- quantile(boston_scaled$crim)
# create label names
crim_lab <- c("low", "med_low", "med_high", "high")
# create new factor variable
crime <- cut(boston_scaled$crim, breaks = bins, include.lowest = TRUE, labels = crim_lab)
# add the new variable to the data frame and remove the old one
boston_scaled$crime <- crime
boston_scaled <- boston_scaled %>%
  select(-crim)
# summary of the new variable
table(boston_scaled$crime)

'# draw a bar plot of the variable in the new data set
boston_scaled %>%
  select(-crime) %>%
  gather() %>%
  ggplot(aes(value)) + 
  facet_wrap("key", scales = "free") +
  geom_histogram()'

# pick the number of total observations in the data
n <- nrow(boston_scaled)
# take a sample of 80% observations; i.e. pick randomly row numbers between 1 and 0.8 x rows
train_indexes <- sample(n, size = n * 0.8)
# create the training set by using the defined indexes for rows
train_set <- boston_scaled[train_indexes,]
# create the testing set by excluding the row used for training set
test_set <- boston_scaled[-train_indexes,]
```


## 4.4 Linear discriminant analysis (LDA)

*Fit the linear discriminant analysis on the train set. Use the categorical crime rate as the target variable and all the other variables in the dataset as predictor variables. Draw the LDA (bi)plot.*

Relating this there was multiple errors and the video was not available in the Datacamp exercises. I found the instructions also rather poor. The book wasn't much of help for this LDA either. Anycase, let's fit the LDA and use 'crime' variable as the target variable and all the other variables as predictors. Below I show the results in a table and in a biplot. No interpretation were asked regarding this in the instructions, so none provided.


```{r}
# fit the linear discriminant analysis
lda.fit <- lda(crime ~ ., data = train_set)
# print the LDA results
lda.fit
# draw LDA biplot
classes <- as.numeric(train_set$crime)
plot(lda.fit, dimen = 2, col = classes, pch = classes)

```

Next, I will save the observed frequencies of the crime categories from the test data and then remove the variable, after which I test how well the fitted model predicts the values. The results are shown below. The classifier, that is the fitted model, did not predict the crime rates perfectly, alghouth the predictions are somewhat accurate. Most of the predictions matches the observed classes, and those that do not match falls mostly to the next classes (i.e. prediction are not right but close).


```{r}
# save the crime variable separately
correct_cases <- test_set$crime
# remove crime variable from the test data
test_set <- select(test_set, -crime)
# use LDA model for predicting crime cases
lda.predict <- predict(lda.fit, newdata = test_set)
# cross tabulate the correct cased with predictions
table(correct = correct_cases, predicted = lda.predict$class)

```

## 4.5 K-means clustering 

*Reload the Boston dataset and standardize the dataset (we did not do this in the Datacamp exercises, but you should scale the variables to get comparable distances). Calculate the distances between the observations. Run k-means algorithm on the dataset. Investigate what is the optimal number of clusters and run the algorithm again. Visualize the clusters (for example with the pairs() or ggpairs() functions, where the clusters are separated with colors) and interpret the results. (0-4 points)*

```{r}
# clean the environment (i.e. data.frames and variables etc.)
rm(list = ls())

# reload the Boston dataset
data(Boston)

# scale the Boston data and transform the matrix to data.frame
boston_scaled <- Boston %>%
  scale() %>%
  as.data.frame()

dist_euc <- dist(boston_scaled, method = "euclidean")
summary(dist_euc)
```

Here I reloaded the Boston data and recreated a new data set, in which the variables are standardized to be able to compare the distances. Next, I calculated the 'euclidean' distances between observations. The key values of the calculated distances are shown in the summary table above.

