# Chapter 4: Clustering and classification


```{r}
date()

```

**_My fourth week in short_**

- The week started by doing the peer-reviews
- For warming up I did first the data wrangling exercise (preparing the data for the 5th week exercise)
- Next I started to read the related chapters and figuring out the DataCamp exercises (and their bugs!)
- Finally I did the RStudio exercises
- I found this week's exercises a bit annoying and uninspiring as I think the study materials did not provide adequate information to work with the exercises or interpret the results

## 4.1 Data
```{r message=FALSE, warning=FALSE}
# access to packages
library(MASS)
library(dplyr)
library(tidyr)
library(ggplot2)
library(corrplot)

# get the data (included in MASS)
data(Boston)

# structure of the data
str(Boston)
dim(Boston)

```

The Boston data set utilized here includes 'Housing Values in Suburbs of Boston', that is different variables relating housing in the city of Boston. The data consists of 506 observations of 14 numeric (or integer) variables (i.e. 506 rows and 14 columns). Description of the variables can be seen from [the description of the data set](https://stat.ethz.ch/R-manual/R-devel/library/MASS/html/Boston.html)

## 4.2 Overview of the Boston data

```{r}
summary(Boston)

# let's use gather from tidyr-package; gather returns key-value pairs of variables
# draw a bar plot of each variable
gather(Boston) %>% 
  ggplot(aes(value)) + 
  facet_wrap("key", scales = "free") +
  geom_histogram()

# construct a correlation matrix and round the results
cor_matrix <-cor(Boston) %>%
    round(digits = 2)

# visualize the correlation matrix
corrplot(cor_matrix, method = "circle", type = "upper", cl.pos = "r", tl.pos = "d", tl.cex = 0.8)

```

Summaries of the variables and the histograms presented above illustrate that the variables in the data set have quite skewed distributions and that the variables also have quite different scales compared to each other. 

The correlation matrix shows that many of variables are highly correlated. The correlation coefficients vary from 1 to -1. A positive coefficient indicate that high values of variable 'X' are associated with the high values of variable 'Y'. Respectively negative coefficient indicates that high values of 'X' are associated with low values of 'Y'.

## 4.3 Standardized dataset 

First I'll scale the Boston data, that is standardize each variable by its scale and save the standardized variables as a data.frame instead of a matrix. When variables are centered, their mean is adjusted to zero as can be seen from the summaries of the scaled variables which are shown below.

After scaling the data, I will create a new factor variable from the 'crime rate per capita' variable and use the quantiles as cut points. A summary of the new 'crime' variable can be found below.

Finally, I will split the scaled data into a training and testing data sets, which are then used in the next subchapter.

```{r}

# scale the Boston data and transform the matrix to data.frame
boston_scaled <- Boston %>%
  scale() %>%
  as.data.frame()
summary(boston_scaled)

# pick the quantiles of crim
bins <- quantile(boston_scaled$crim)
# create label names
crim_lab <- c("low", "med_low", "med_high", "high")
# create new factor variable
crime <- cut(boston_scaled$crim, breaks = bins, include.lowest = TRUE, labels = crim_lab)
# add the new variable to the data frame and remove the old one
boston_scaled$crime <- crime
boston_scaled <- boston_scaled %>%
  select(-crim)
# summary of the new variable
table(boston_scaled$crime)


# pick the number of total observations in the data
n <- nrow(boston_scaled)
# take a sample of 80% observations; i.e. pick randomly row numbers between 1 and 0.8 x rows
train_indexes <- sample(n, size = n * 0.8)
# create the training set by using the defined indexes for rows
train_set <- boston_scaled[train_indexes,]
# create the testing set by excluding the row used for training set
test_set <- boston_scaled[-train_indexes,]
```


## 4.4 Linear discriminant analysis (LDA)

Relating this part of the exercise there was multiple problems with the DataCamp exercises and the video was not available. Anycase, let's fit the LDA and use 'crime' variable as the target variable and all the other variables as predictors. Below I show the results in a table and in a biplot. No interpretation were asked regarding this in the instructions, so none provided.


```{r}
# fit the linear discriminant analysis
lda.fit <- lda(crime ~ ., data = train_set)
# print the LDA results
lda.fit
# draw LDA biplot
classes <- as.numeric(train_set$crime)
plot(lda.fit, dimen = 2, col = classes, pch = classes)

```

Next, I will save the observed frequencies of the crime categories from the test data and then remove the variable, after which I test how well the fitted model predicts the values. The results are shown below. The classifier, that is the fitted model, did not predict the crime rates perfectly, alghouth the predictions are somewhat accurate. Most of the predictions matches the observed classes, and those that do not match falls mostly to the next classes (i.e. prediction are not right but close).


```{r}
# save the crime variable separately
correct_cases <- test_set$crime
# remove crime variable from the test data
test_set <- select(test_set, -crime)
# use LDA model for predicting crime cases
lda.predict <- predict(lda.fit, newdata = test_set)
# cross tabulate the correct cased with predictions
table(correct = correct_cases, predicted = lda.predict$class)

```

## 4.5 K-means clustering 

Here, I reloaded the Boston data and recreated a new data set, in which the variables are standardized to be able to compare the distances. Next, I calculate the 'euclidean' distances between observations. A summary of the calculated distances are shown in the table above. 

```{r}
# clean the environment (i.e. data.frames and variables etc.)
rm(list = ls())

# reload the Boston dataset
data(Boston)

# scale the Boston data and transform the matrix to data.frame
boston_scaled <- Boston %>%
  scale() %>%
  as.data.frame()

dist_euc <- dist(boston_scaled, method = "euclidean")
summary(dist_euc)

```
After that K-means clustering is conducted. First with three centers and then after examining the within cluster sum of squares (WCSS) K-means clustering is run again with two centers that was suggested by the results of the investigation of the WCSS. Below the results from two-center-clustering are visualized in the scatter plot matrices in which the clusters are colored; first plot shows all of the variables (which is quite useless) and then the three plots show parts of the big picture; all of them the two more or less distinct cluster are visible.

```{r message=FALSE, warning=FALSE}

# k-mean clustering with 4 centers
km_boston <- kmeans(boston_scaled, centers = 3)

# examination of WCSS, that is deciding optomal number of centers
# set seed
set.seed(123)
# determine the number of clusters in the examination
k_max <- 10
# calculate the total WCSS
twcss <- sapply(1:k_max, function(k){kmeans(boston_scaled, k)$tot.withinss})
# visualize the results
qplot(x = 1:k_max, y = twcss, geom = 'line')

# examination of WCSS and the visualization suggest that two center would be optimal
km_boston <- kmeans(boston_scaled, centers = 2)

# visualze the k-mean clustering results
pairs(boston_scaled, col = km_boston$cluster)
pairs(boston_scaled[1:5], col = km_boston$cluster)
pairs(boston_scaled[6:10], col = km_boston$cluster)
pairs(boston_scaled[11:14], col = km_boston$cluster)
```


**_THAT'S ALL FOR THIS TIME!_**
