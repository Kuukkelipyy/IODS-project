# Chapter 3: Logistic regression



```{r}
date()

```

**_My third week in short_**

- I started the week's work by reading the chapters 5 and 6 of the book *Multivariate Analysis for the Behavioral science* 
- While reading the book I did the Datacamp exercises
- After the above described studying I started to work with the RStudio exercises, that is Data wrangling and analyzing the data
- Analysis and interpretation of the results are reported below

## 3.1 Data

In this exercise the data is drawn from the [Student Performance Data](https://archive.ics.uci.edu/ml/datasets/Student+Performance), which includes data on student achievements in secondary education of two Portuguese schools.  

``` {r message = FALSE, warning = FALSE}
# access to all packages needed
library(dplyr)
library(ggplot2)
library(GGally)
library(tidyr)
library(ggpubr)
library(boot)

# read the prepared data set from local file
alc <- read.table("data/alc.csv", sep = ";")

# examine the data
dim(alc)
colnames(alc)

```
After data preparations, the data consist of 370 observations (rows) of 35 variables (columns), that is information from 370 respondents regarding 35 variables. More info regarding data wrangling can be found from this R-Script-file. The names of the variable are shown above and more detailed description of them is presented in [the webpage of the original data](https://archive.ics.uci.edu/ml/datasets/Student+Performance) 

### 3.2.1 Selected variables and hypotheses

After examining the variables in the data I chose the following four variables for further examination as I assume they could be associated with the level of alcohol consumption:

1) sex; student's gender (binary: 'F' - female or 'M' - male) 
2) romantic; with a romantic relationship (binary: yes or no) 
3) studytime; weekly study time (numeric: 1 - <2 hours, 2 - 2 to 5 hours, 3 - 5 to 10 hours, or 4 - >10 hours) 
4) goout; going out with friends (from 1 (very low) to 5 (very high))

My hypotheses regarding each of the above presented variables are:

1) Male students are more likely to be high users of alcohol compared to females
2) Students with a romantic relationship are less likely to be high users compared to those you are not in a relationship
3) Students who spend more time studying are less likely be high users of alcohol than those who study less
4) Students who go out with their friends more often are more likely to be high users of alcohol than the students who go out less often

### 3.2.2 Overview of the selected variables

Below I present the summary tables as well as bar plots of each variable of interest. They show the distributions of the variables in question.

```{r message=FALSE, warning=FALSE}

# before examining the variables I'll transform the two character variables of interest to factors
alc <- mutate(alc, sex = as.factor(sex), romantic = as.factor(romantic))

# pick the names of the variables of interest
varnames <- select(alc, sex, romantic, studytime, goout, high_use) %>%
  colnames()

# summary of each variable
select(alc, varnames) %>%
  summary()

# a bar plot of each variable
gather(alc[varnames]) %>% ggplot(aes(value)) + 
  facet_wrap("key", scales = "free") +
  geom_bar()

```

### 3.2.3 Alcohol consumpion and selected variables 

Below I presented a descriptive examination on how the selected variables are related with alcohol consumption. First all variables of interest are cross-tabulated with the 'high use of alcohol' variables, after which the figure show the proportional distributions of high users.

```{R}

# cross-tabulations with high_use:

# sex and high_use
addmargins(table(alc$sex, alc$high_use))

# romantic and high_use
addmargins(table(alc$romantic, alc$high_use))

# high_use and studytime
addmargins(table(alc$high_use, alc$studytime))

# high_use and goout
addmargins(table(alc$high_use, alc$goout))

# proportion figures

t1 <- ggplot(data = alc, aes(x = sex, fill = high_use)) +
  geom_bar(position = "fill") +
  scale_y_continuous(labels = scales::percent) +
  ylab("%") +
  xlab("Gender")

t2 <- ggplot(alc, aes(romantic, fill = high_use)) +
  geom_bar(position = "fill") +
  scale_y_continuous(labels = scales::percent) +
  ylab("%") +
  xlab("Romantic relationship")

t3 <- ggplot(alc, aes(x = goout, fill = high_use)) + 
  geom_bar(position = "fill") +
  scale_y_continuous(labels = scales::percent) +
  ylab("%") +
  xlab("Go out with friends")

t4 <- ggplot(alc, aes(x = studytime, fill = high_use)) + 
  geom_bar(position = "fill") +
  scale_y_continuous(labels = scales::percent) +
  ylab("%") +
  xlab("Studytime")

# all plots in one figure
ggarrange(t1 + rremove("legend"), t2 + rremove("legend"), t3 + rremove("legend"), t4 + rremove("legend"),
          ncol = 2, nrow = 2, 
          common.legend = TRUE, legend = "bottom")


```

The tables and figure above indicate that the are some differences in the alcohol consumption regarding the variables in examination.  


## 3.3 Logistic regression

```{r}
# logistic regression model
model1 <- glm(high_use ~ sex + romantic + studytime + goout, data = alc, family = "binomial")

# summary of the results
summary(model1)

```
Interpretation of the above results.."Present and interpret the coefficients of the model as odds ratios and provide confidence intervals for them. Interpret the results and compare them to your previously stated hypothesis"

```{r}

# compute odds ratios (OR)
OR <- coef(model1) %>% 
  exp

# compute confidence intervals (CI) for the odds ratios
CI <- confint(model1) %>%
    exp

# combine the odds ratios and their confidence intervals
OR_with_CI <- cbind(OR, CI)
# round the results 
round(OR_with_CI, digits = 2)

```
### Predictions

Using the variables which, according to your logistic regression model, had a statistical relationship with high/low alcohol consumption, explore the predictive power of you model. 
- Provide a 2x2 cross tabulation of predictions versus the actual values and 
- optionally display a graphic visualizing both the actual values and the predictions. 
- Compute the total proportion of inaccurately classified individuals (= the training error) and comment on all the results. Compare the performance of the model with performance achieved by some simple guessing strategy

According to the above-presented model all variables except 'romance' had statistically significant relationship with high/low alcohol consumption. Next, we examine the accurate the model predictions are.

```{r}

# new logistic regression model
model2 <- glm(high_use ~ sex + studytime + goout, data = alc, family = "binomial")

# predicted probabilities and prediction (> 0.5) of high_use and add it to the data.frame
probabilities <- predict(model2, type = "response")
alc <- mutate(alc, probability = probabilities)
alc <- mutate(alc, prediction = probability > 0.5)

# tabulate the observed high use versus the predictions
table(high_use = alc$high_use, prediction = alc$prediction)

# probability table of high use vs. predictions
prob_table <- table(high_use = alc$high_use, prediction = alc$prediction) %>%
    prop.table %>%
    addmargins
round(prob_table * 100, digits = 2) # probs in % and rounded

ggplot(alc, aes(x = probability, y = high_use, col = prediction)) +
  geom_point(alpha = 0.5, size = 3)

# define a loss function (mean prediction error)
loss_func <- function(class, prob) {
  n_wrong <- abs(class - prob) > 0.5
  mean(n_wrong)
}

# Calculate the share of false predictions (false positives + false negatives)
## this can be seen from the above prob.table as well
loss_func(class = alc$high_use, prob = alc$probability)

```
First table provides shows cross-tabulation of predictions against the actual values of high_use, and the next table shows the proportions of each cell. According to the proportion table about 77.6 % of predictions (cells: False/false + True/true) match with actual values of observations, while 22,4 % of the predictions (cells: True/False + False/True) are false. This is also confimed by using the self-defined 'loss function' to see the share of false predictions, which is 22,4 %.


## 3.4 Cross-validation

### 10-fold cross-validation of the model (bonus)

```{r}

# compute the average number of wrong predictions in the (training) data
## loss func is defined in previous chunk
nwrong_train <- loss_func(class = alc$high_use, prob = alc$probability)
# mean error in in the training data
nwrong_train

# 10-fold cross-validation
crossvalid <- cv.glm(data = alc, cost = loss_func, glmfit = model2, K = 10)
# the mean prediction error for the testing data
crossvalid$delta[1]

```

The mean error in the training data is 0.224. The mean prediction error for the testing data is around 0.24 in the 10-fold cross-validation. The mean prediction error in the Datacamp model was about 0.26, suggesting that the model presented here has slightly better test performance; i.e. the model is more accurate predicting the high consumption of alcohol.

### Finding more parsimonious model (super-bonus)

Here I utilize AIC (*Akaike Information Criteria*) backward elimination -procedure for selecting the explanatory variables for the model. I start by selecting 10 variables and from there start to elimination. 

```{r}
# logistic regression model
model3 <- glm(high_use ~ sex + age + Pstatus + absences + failures + schoolsup + studytime + goout + activities + freetime, data = alc, family = "binomial")

step(model3, direction = "backward")
```
The backward elimination suggest that we should keep five of those original 10 variables; sex, failures, activities, studytime, absences and goout. Thus, I will run the logistic model with those variables and conduct the 10-fold cross-validation.

```{r}

model4 <- glm(high_use ~ sex + failures + activities + studytime + absences + goout,
              data = alc, family = "binomial")

summary(model4)
# 10-fold cross-validation
crossvalid2 <- cv.glm(data = alc, cost = loss_func, glmfit = model4, K = 10)
# the mean prediction error for the testing data
crossvalid2$delta[1]

# visual
probabilities_m4 <- predict(model4, type = "response")
alc <- mutate(alc, probability_m4 = probabilities_m4)
alc <- mutate(alc, prediction_m4 = probability_m4 > 0.5)

ggplot(alc, aes(x = probability_m4, y = high_use, col = prediction_m4)) +
  geom_point(alpha = 0.5, size = 3)


```
The table above present the summary of fourth model. Although all of them are not statistically significant I keep them in the model as suggested by previously conducted backward selection. According to the 10-fold cross-validation, the mean prediction error for the testing data is around 0.21, suggesting that this model has better test performance than the model 3 examined above. The figure provides of graphical confirmation of this assumption. 
