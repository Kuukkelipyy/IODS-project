# Chapter 3: Logistic regression



```{r}
date()

```

**_My third week in short_**

- I started the week's work by reading the chapters 5 and 6 of the book *Multivariate Analysis for the Behavioral science* 
- While reading the book I did the Datacamp exercises
- After the above described studying I started to work with the RStudio exercises, that is Data wrangling and analyzing the data
- Analysis and interpretation of the results are reported below

## 3.1 Data

``` {r message = FALSE, warning = FALSE}
# access to all packages needed
library(dplyr)
library(ggplot2)
library(GGally)
library(tidyr)
library(ggpubr)
library(boot)

# read the prepared data set from local file
alc <- read.table("data/alc.csv", sep = ";")

# examine the data
dim(alc)
colnames(alc)

```
In this exercise the data is drawn from the [Student Performance Data set](https://archive.ics.uci.edu/ml/datasets/Student+Performance), which includes information on student achievements in secondary education of two Portuguese schools. After data wrangling, the data set used here consist of 370 observations (rows) of 35 variables (columns), that is information from 370 respondents regarding 35 variables. The names of the variables included in the prepared data set are shown above and more detailed description of them is presented in [the description of the Student Performance Data](https://archive.ics.uci.edu/ml/datasets/Student+Performance). 

## 3.2 Selected variables and hypotheses

After examining the variables in the data I chose the following four variables for further examination as I assume they could be associated with the level of alcohol consumption:

1) **_sex_**; student's gender (F = female, M = male) 
2) **_romantic_**; with a romantic relationship (yes or no) 
3) **_studytime_**; weekly study time (numeric: 1 = less than 2 hours, 2 = 2 to 5 hours, 3 = 5 to 10 hours, 4 = over 10 hours) 
4) **_goout_**; going out with friends (from 1 (very low) to 5 (very high))

My hypotheses are:

1) Male students are more likely to be high users of alcohol compared to females
2) Students with a romantic relationship are less likely to be high users compared to those you are not in a relationship
3) Students who spend more time studying are less likely be high users of alcohol than those who study less
4) Students who go out with their friends more often are more likely to be high users of alcohol than the students who go out less often

### 3.2.1 Overview of the selected variables

Below I present the summary tables as well as plots of each variable of interest.

```{r message=FALSE, warning=FALSE}

# before examining the variables I'll transform the two character variables of interest to factors
alc <- mutate(alc, sex = as.factor(sex), romantic = as.factor(romantic))

# pick the names of the variables of interest
varnames <- select(alc, sex, romantic, studytime, goout, high_use) %>%
  colnames()

# summary of each variable
select(alc, varnames) %>%
  summary()

# a plot of each variable
gather(alc[varnames]) %>% ggplot(aes(value)) + 
  facet_wrap("key", scales = "free") +
  geom_bar()

```

### 3.2.2 Alcohol consumpion and selected variables 

Next, I examine how the selected variables are related with alcohol consumption. First, all variables of interest are tabulated with the 'high use of alcohol' variable, after which the figure shows the proportional distributions of high users regarding each variable.

```{R}

# cross-tabulations with high_use:

# sex and high_use
addmargins(table(alc$sex, alc$high_use))

# romantic and high_use
addmargins(table(alc$romantic, alc$high_use))

# high_use and studytime
addmargins(table(alc$high_use, alc$studytime))

# high_use and goout
addmargins(table(alc$high_use, alc$goout))

# proportion figures

t1 <- ggplot(data = alc, aes(x = sex, fill = high_use)) +
  geom_bar(position = "fill") +
  scale_y_continuous(labels = scales::percent) +
  ylab("%") +
  xlab("Gender")

t2 <- ggplot(alc, aes(romantic, fill = high_use)) +
  geom_bar(position = "fill") +
  scale_y_continuous(labels = scales::percent) +
  ylab("%") +
  xlab("Romantic relationship")

t3 <- ggplot(alc, aes(x = goout, fill = high_use)) + 
  geom_bar(position = "fill") +
  scale_y_continuous(labels = scales::percent) +
  ylab("%") +
  xlab("Go out with friends")

t4 <- ggplot(alc, aes(x = studytime, fill = high_use)) + 
  geom_bar(position = "fill") +
  scale_y_continuous(labels = scales::percent) +
  ylab("%") +
  xlab("Studytime")

# all plots in one figure ('ggarrange' is from 'ggpubr' package)
ggarrange(t1 + rremove("legend"), t2 + rremove("legend"), t3 + rremove("legend"), t4 + rremove("legend"),
          ncol = 2, nrow = 2, 
          common.legend = TRUE, legend = "right")


```

The tables and figure above indicate that bigger share of males are high users of alcohol compared to females. Similarly, the more the student go out with friends or the less time they use for studying, the bigger is the share of high users. In contrast, the share of high users of alcohol is quite the same among those who are in a romantic relationship and those who are not. Thus, these descriptive results provide support for the above-presented hypotheses 1, 3 and 4, while hypothesis 2 is rather questionable. 

## 3.3 Logistic regression

```{r}
# logistic regression model
## studytime is used as factors in the model
model1 <- glm(high_use ~ sex + romantic + goout + as.factor(studytime), data = alc, family = "binomial")

# summary of the results
summary(model1)

# compute odds ratios (OR)
OR <- coef(model1) %>% 
  exp

# compute confidence intervals (CI) for the odds ratios
CI <- confint(model1) %>%
    exp

# combine the odds ratios and their confidence intervals
OR_with_CI <- cbind(OR, CI)
# round the results 
round(OR_with_CI, digits = 2)

```

The results from a logistic regression model are presented above. The value of null deviance is for a model without any explanatory variables and the residual deviance is the value when taking all explanatory variables into account. Higher numbers indicate bad fit, and respectively smaller values indicate good fit. The difference between null deviance and residual deviance illustrates how good is the fit of the given model with explanatory variables; the greater the difference, the better. Although the value of the residual deviance is quite high, the difference shows that the model with explanatory variables is more fit than a model including only the intercept term. The value of AIC (Akaikeâ€™s information criteria) can be used for comparing competing models (this will be discussed more below).

The coefficients illustrates the association between the explanatory variables and the target variables. The estimates can be also used, for instance, for calculating predicted probabilities of being a high user based on the known values of explanatory values. According to the coefficient table having a romantic relationship is not significantly associated with the use of alcohol. Instead, the results show that gender, time used for studying and going out with friends are statistically significant variables predicting high consumption of alcohol. However, all the associations are not rectilinear, which will be discussed along with the odds ratios.

The odds ratio (OR) illustrates the difference in the odds of being high user between groups. For instance, in the case of sex variable females are the reference group (OR = 1.00) and the odds of males are compared to the odds of females; the odds ratio shows that males (OR 2.03) are about twice as likely to be high users compared to females. Going out with friends more often increase (OR = 2.10) the likelihood of being high user compared to those who goes out the least often (note that the actual scale of the variable is somewhat obscure, and thus difficult to interpret). Finally, compared to students who study less than two hour, those who study 5 to 10 hours are about three times (OR = 0.34) less likely to be high users and respectively those who study over 10 hours (OR = 0.28) are about 3.5 times less likely to be high users. Instead, those who study 2-5 hours are not statistically more or less likely to be high users compared to the reference group (the coefficient is not statistically significant and the CI's of OR include 0). 

#### Predictions

According to the above-presented model all variables except 'romance' had statistically significant relationship with high/low alcohol consumption. Next, we build a new regression model without the romance variable and examine how accurate the model predictions are.

```{r}

# new logistic regression model
model2 <- glm(high_use ~ sex + as.factor(studytime) + goout, data = alc, family = "binomial")

# predicted probabilities and prediction (> 0.5) of high_use and add it to the data.frame
probabilities <- predict(model2, type = "response")
alc <- mutate(alc, probability = probabilities)
alc <- mutate(alc, prediction = probability > 0.5)

# tabulate the observed high use versus the predictions
table(high_use = alc$high_use, prediction = alc$prediction)

# probability table of high use vs. predictions
prob_table <- table(high_use = alc$high_use, prediction = alc$prediction) %>%
    prop.table %>%
    addmargins
round(prob_table * 100, digits = 2) # probs in % and rounded

ggplot(alc, aes(x = probability, y = high_use, col = prediction)) +
  geom_point(alpha = 0.5, size = 3)

# define a loss function (mean prediction error)
loss_func <- function(class, prob) {
  n_wrong <- abs(class - prob) > 0.5
  mean(n_wrong)
}

# Calculate the share of false predictions (false positives + false negatives)
## this can be seen from the above prob.table as well
loss_func(class = alc$high_use, prob = alc$probability)

```
The first table provides cross-tabulation of the predictions against the actual values of high_use, and the next table shows the proportions of each cell. According to the proportion table about 77.6 % of predictions (cells: False/false + True/true) match with actual values of observations, while 22,4 % of the predictions (cells: True/False + False/True) are incorrect. This is also confirmed by using the self-defined 'loss function' to see the share of false predictions, which is 22,4 %. The plot should visualize the results, although it seems there is something wrong with the figure (but I can't find an error in the code).

## 3.4 Cross-validation

### 3.4.1 10-fold cross-validation of the model (bonus)

```{r}

# compute the average number of wrong predictions in the (training) data
## loss func is defined in previous chunk
nwrong_train <- loss_func(class = alc$high_use, prob = alc$probability)
# mean error in in the training data
nwrong_train

# 10-fold cross-validation
crossvalid <- cv.glm(data = alc, cost = loss_func, glmfit = model2, K = 10)
# the mean prediction error for the testing data
crossvalid$delta[1]

```

The mean error in the training data is 0.224. The mean prediction error for the testing data is around 0.24 in the 10-fold cross-validation. The mean prediction error in the Datacamp model was about 0.26, suggesting that the model presented here has slightly better test performance; i.e. the model is more accurate predicting the high consumption of alcohol.

### 3.4.2 Finding more parsimonious model (super-bonus)

Here I utilize the AIC (Akaike's Information Criteria) backward elimination -procedure for selecting the explanatory variables for the model. The AIC index takes into account the statistical goodness of fit and the number of variables in the model by increasing a penalty for a greater number of variables. In the series of competing models lower AIC values are preferred; i.e. the aim is to achieve as low AIC value as possible by excluding variables - that makes the value higher - one at the time. The elimination ends when removing more variables would not improve the AIC score. I start by selecting 10 variables and from there start the elimination. 

```{r}

# logistic regression model
model3 <- glm(high_use ~ sex + age + Pstatus + absences + failures + schoolsup + as.factor(studytime) + goout + activities + freetime, data = alc, family = "binomial")

step(model3, direction = "backward")
```
The backward elimination suggest that we should keep five of those original 10 variables; sex, failures, activities, absences and goout. Interestingly, studytime was excluded! Next, I will run the logistic model with those variables and conduct the 10-fold cross-validation.

```{r}

model4 <- glm(high_use ~ sex + failures + activities + absences + goout,
              data = alc, family = "binomial")

summary(model4)
# 10-fold cross-validation
crossvalid2 <- cv.glm(data = alc, cost = loss_func, glmfit = model4, K = 10)
# the mean prediction error for the testing data
crossvalid2$delta[1]

# visual
probabilities_m4 <- predict(model4, type = "response")
alc <- mutate(alc, probability_m4 = probabilities_m4)
alc <- mutate(alc, prediction_m4 = probability_m4 > 0.5)

ggplot(alc, aes(x = probability_m4, y = high_use, col = prediction_m4)) +
  geom_point(alpha = 0.5, size = 3)


```

The table above presents the summary of the fourth model. According to the 10-fold cross-validation, the mean prediction error for the testing data is around 0.20, suggesting that this model has better test performance than the model 3 examined above. The figure provides of graphical confirmation/evidence for this assumption. Moreover, the value of residual deviance (smaller) and the difference between null vs. residual deviance (bigger) suggest that this new model is better than the previous one.


**_Thanks for reading and for the upcoming feedback!_**